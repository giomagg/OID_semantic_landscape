{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69567a44",
   "metadata": {},
   "source": [
    "# OID Semantic Landscape – Full Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF, PDFs, Numpy, and regular expressions\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import fitz \n",
    "\n",
    "# Text Splitting\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings, dimensionality reduction, and clustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "\n",
    "# Interpretation\n",
    "from LengthSafeClusterThemeExtractor import ClusterThemeExtractor\n",
    "\n",
    "# Visualization\n",
    "import datamapplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a1129",
   "metadata": {},
   "source": [
    "# Step 1 - Cleaning and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d1c8b",
   "metadata": {},
   "source": [
    "Import DataFrame and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset includes all PDFs from Zotero and all the additional ones we gathered\n",
    "df=pd.read_csv('.../map_df_FINAL.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = df[df['File Attachments'].notna()]['File Attachments']\n",
    "paths = paths.str.split(';')\n",
    "paths = paths.explode()\n",
    "paths = paths[paths.str.contains('.pdf')] # select only those referring to pdfs\n",
    "paths = [s[1:] if s.startswith(\" \") else s for s in paths] # There was an annoying space at the beginning of some PDF paths so I took it away when present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10331340",
   "metadata": {},
   "source": [
    "Trim Footnotes and Trim Bibliography Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_bibliography(page_content):\n",
    "    # Case-insensitive search for bibliography/references\n",
    "    pattern = r'\\b(references|bibliography)\\b'\n",
    "    match = re.search(pattern, page_content, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # Only trim from the first occurrence of references/bibliography\n",
    "        return page_content[:match.start()].strip()\n",
    "    \n",
    "    return page_content\n",
    "\n",
    "###########################\n",
    "def remove_footnotes(text):\n",
    "    # Regular expression to match footnote patterns\n",
    "    footnote_pattern = r'\\[\\d+\\]|\\(\\d+\\)|\\d+\\.'\n",
    "    \n",
    "    # Remove footnote markers\n",
    "    text_without_footnotes = re.sub(footnote_pattern, '', text)\n",
    "    \n",
    "    return text_without_footnotes.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d25e39",
   "metadata": {},
   "source": [
    "### Read and Chunk PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the TextSplitter from LangChain\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1300,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c298fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize dataframe\n",
    "chunk_df = pd.DataFrame(columns=['Publication Year', 'Authors', 'Title', 'Source', 'chunk_number', 'text'])\n",
    "failed_paths = []\n",
    "\n",
    "# Iterate over each path we have in our Zotero Library    \n",
    "for path in paths:\n",
    "    try:\n",
    "         # Extract key variables that we need to know what chunk is belonging to what article\n",
    "        matching_rows = df[df['File Attachments'].str.contains(path, na=False)]\n",
    "\n",
    "        if len(matching_rows) == 0:\n",
    "            print(f\"No matching entry found for path: {path}\")\n",
    "            print()\n",
    "            failed_paths = failed_paths.append(path)\n",
    "            continue\n",
    "        \n",
    "        # Extract key variables\n",
    "        year = matching_rows['Publication Year'].values[0]\n",
    "        author = matching_rows['Author'].values[0]\n",
    "        title = matching_rows['Title'].values[0]\n",
    "        source = matching_rows['Journal Abbreviation'].values[0]\n",
    "        \n",
    "        # Initialize empty text string\n",
    "        full_text = \"\"\n",
    "\n",
    "        # Open PDF File\n",
    "        with fitz.open(path) as pdf_document:\n",
    "            print(f'Processing {title} with {pdf_document.page_count} pages...')\n",
    "\n",
    "            # Iterate through the pages and extract text\n",
    "            for page_num in range(pdf_document.page_count):\n",
    "                page = pdf_document[page_num]\n",
    "                page_content = page.get_text()\n",
    "                \n",
    "                # Remove footnotes\n",
    "                page_content = remove_footnotes(page_content)\n",
    "\n",
    "                # Put it all together in one big text string for the full article\n",
    "                full_text += page_content + \" \"\n",
    "\n",
    "            # Remove bibliography \n",
    "            full_text = trim_bibliography(full_text)\n",
    "            print('Full text acquired, splitting into chunks...')\n",
    "\n",
    "            # Apply the text splitter from langchain\n",
    "            chunks = rec_text_splitter.split_text(full_text)\n",
    "            print(f'The Article produced a total of ', len(chunks), ' chunks.')\n",
    "\n",
    "            # Calculate average length of the chunks\n",
    "            temp = [len(ele) for ele in chunks]\n",
    "            res = 0 if len(temp) == 0 else (float(sum(temp)) / len(temp))        \n",
    "            print(f'The average lenth of a chunk is of {res} characters')\n",
    "\n",
    "            # Iterate through chunks and append them to the main output dataframe\n",
    "            for i, _ in enumerate(chunks):\n",
    "                chunk_number = i\n",
    "                chunk_text = chunks[i]\n",
    "\n",
    "                temp_df = pd.DataFrame({\n",
    "                            'Publication Year': [year],\n",
    "                            'Authors': [author],\n",
    "                            'Title': [title],\n",
    "                            'Source': [source],\n",
    "                            'chunk_number': [chunk_number],\n",
    "                            'text': [chunk_text]\n",
    "                        })\n",
    "\n",
    "                chunk_df = pd.concat([chunk_df, temp_df], ignore_index=True)\n",
    "\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing path {path}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2194261",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have a total of {len(chunk_df)} chunks from {len(chunk_df.Title.unique())} articles')\n",
    "chunk_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.to_csv('report_literature_chunked.zip', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaef076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'In total we failed to process {len(failed_paths)} PDF files for the following paths' \n",
    "      f'which resulted like having no entries in the main df: {failed_paths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94c03a",
   "metadata": {},
   "source": [
    "# Step 2 - Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22624c9e",
   "metadata": {},
   "source": [
    "## First Embedding Step\n",
    "In this first embedding step we calculate **an embedding vector for each chunk** we derived from the text, we then reduce dimensionality and run HDBSCAN for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270e9d1",
   "metadata": {},
   "source": [
    "#### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = chunk_df[~chunk_df['text'].isna()]['text']\n",
    "documents=list(documents.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1513de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fe172",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = dict(zip(chunk_df.index, embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c5c34",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba447d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components = 2, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c06bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embedding = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_dict = dict(zip(chunk_df.index, reduced_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854c064",
   "metadata": {},
   "source": [
    "#### Clusterisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bfc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = clusterer.fit_predict(reduced_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = dict(zip(chunk_df.index, cluster_labels))\n",
    "string_labels = cluster_labels.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64326805",
   "metadata": {},
   "source": [
    "#### Match with chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df['map_cluster']=chunk_df.index.map(clusters_dict)\n",
    "chunk_df['map_coordinates']=chunk_df.index.map(coordinates_dict)\n",
    "chunk_df['embedding_vector']=chunk_df.index.map(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94b769",
   "metadata": {},
   "source": [
    "## Second Embedding Step\n",
    "In the second embedding step we use leverage the clustering from step one to calculate **average vectors for each paper having multiple paragraphs in the same cluster**. \n",
    "\n",
    "Specifically: For papers with multiple paragraphs in the same cluster, we average their positions (in the original embedding space), but paragraphs of a same paper, being in different clusters remain distinct. We then run UMAP again on the “averaged paragraphs\" to derive their position in a reduced dimensionality space and re-cluster using HDBSCAN.\n",
    "\n",
    "After evaluation of a first run which still granted lots of paper-specific clusters, **we run this section a second time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9462da4",
   "metadata": {},
   "source": [
    "First Time\n",
    "~~~\n",
    "averaged_df = chunk_df.groupby(['Title', 'map_cluster']).agg({\n",
    "    'embedding_vector': 'mean',\n",
    "    'chunk_number': list,\n",
    "    'Publication Year': 'first',\n",
    "    'Authors': 'first'\n",
    "}).reset_index()\n",
    "~~~\n",
    "$+$ we run the rest of the section untill \"Interpretation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dfdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second Time\n",
    "averaged_df = averaged_df.groupby(['Title', 'map_cluster']).agg({\n",
    "    'embedding_vector': 'mean',\n",
    "    'chunk_number': list,\n",
    "    'Publication Year': 'first',\n",
    "    'Authors': 'first'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f781028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "averaged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afedcd4b",
   "metadata": {},
   "source": [
    "Step 1 - Create a novel array with the averaged vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_averages = np.stack(averaged_df.embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d089e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/giovanni/Desktop/Forum Info and Democracy /Zotero Data/GarganText/embeddings/FINAL_01_pipeline_embeddings_step2.2.npy',\n",
    "       embeddings_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_averages.shape)\n",
    "similarities = model.similarity(embeddings_averages, embeddings_averages)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb8d060",
   "metadata": {},
   "source": [
    "Step 2 - Project the vectors on a bidimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a262415",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embedding_avg = reducer.fit_transform(embeddings_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca19b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embedding_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_avg_dict = dict(zip(averaged_df.index, reduced_embedding_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/giovanni/Desktop/Forum Info and Democracy /Zotero Data/GarganText/embeddings/FINAL_01_pipeline_umap_step2.2.npy',\n",
    "       reduced_embedding_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390edf2",
   "metadata": {},
   "source": [
    "Step 3 - Run HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_avg = clusterer.fit_predict(reduced_embedding_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict_avg = dict(zip(averaged_df.index, cluster_labels_avg))\n",
    "string_labels_avg = cluster_labels_avg.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1be1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/giovanni/Desktop/Forum Info and Democracy /Zotero Data/GarganText/embeddings/FINAL_01_pipeline_cluster_labels_step2.2.npy',\n",
    "        clusters_dict_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303103d",
   "metadata": {},
   "source": [
    "Step 4 - Map onto averaged_df and re-order columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f472a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df['map_cluster']=averaged_df.index.map(clusters_dict_avg)\n",
    "averaged_df['map_coordinates']=averaged_df.index.map(coordinates_avg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df= averaged_df[['Publication Year', 'Authors', 'Title', 'chunk_number',\n",
    "                          'map_cluster', 'map_coordinates', 'embedding_vector']]\n",
    "averaged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624adb0",
   "metadata": {},
   "source": [
    "# Step 3 - Interpretation\n",
    "Interpretation was conducted quantitatively and qualitatively. We started by creating a dataset for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df['chunk_number_flat'] = averaged_df['chunk_number'].apply(lambda x: [item for sublist in x for item in (sublist if isinstance(sublist, list) else [sublist])])\n",
    "\n",
    "chunk_text_mapping = {}\n",
    "for _, row in chunk_df.iterrows():\n",
    "    key = (row['Title'], row['chunk_number'])\n",
    "    chunk_text_mapping[key] = row['text']\n",
    "\n",
    "# Map and merge texts by matching both Title and chunk_number\n",
    "def merge_texts(row):\n",
    "    title = row['Title']\n",
    "    chunk_numbers = row['chunk_number_flat']\n",
    "    texts = [chunk_text_mapping.get((title, num), '') for num in chunk_numbers]\n",
    "    return ' '.join(filter(None, texts))\n",
    "\n",
    "averaged_df['merged_text'] = averaged_df.apply(merge_texts, axis=1)\n",
    "\n",
    "averaged_df = averaged_df.drop(columns=['chunk_number_flat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc4ce4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "averaged_df.to_csv('interpretation_df.csv')\n",
    "averaged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad07aa",
   "metadata": {},
   "source": [
    "### Quantitative Analysis\n",
    "\n",
    "**WARNING**: This section is highly computationally intensive as it leverages BERT to extract the theme of each cluster based on the processing of its text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ClusterThemeExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758a9a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For around 3,300 different chunks it takes about 1 hour and a half to run\n",
    "themes = extractor.extract_cluster_themes(texts_by_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4138e57",
   "metadata": {},
   "source": [
    "### Qualitative Analysis\n",
    "We then analysed the output qualitatively, reading representative paragraphs and full documents where LLM-based labelling was unclear. Random samples of clearly defined clusters were also analysed to check BERT's output. We then developed a **dictionary of intepretation** to use for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"-1\": \"Unlabelled\",\n",
    "    \"0\": \"Zero Rating and Net Neutrality\",\n",
    "    \"1\": \"AI and Data Governance Objectives\",\n",
    "    \"2\": \"Unlabelled\",\n",
    "    \"3\": \"(Digital) Media Literacy\",\n",
    "    \"4\": \"(Digital) Media Literacy\",\n",
    "    \"5\": \"Unlabelled\",\n",
    "    \"6\": \"Unlabelled\",\n",
    "    \"7\": \"Journalists Online Harassment\",\n",
    "    \"8\": \"AI Fairness and AI Bias\",\n",
    "    \"9\": \"AI Fairness and AI Bias\",\n",
    "    \"10\": \"AI Fairness and AI Bias\",\n",
    "    \"11\": \"Children Internet Use\",\n",
    "    \"12\": \"Unlabelled\",\n",
    "    \"13\": \"Mis- and Disinformation\",\n",
    "    \"14\": \"AI News and Journalism\",\n",
    "    \"15\": \"Fact Checking\",\n",
    "    \"16\": \"Fake News and Politicization\",\n",
    "    \"17\": \"Covid-19 Misinformation\",\n",
    "    \"18\": \"Political News Consumption\",\n",
    "    \"19\": \"Socio-demographic Indicators\",\n",
    "    \"20\": \"Media Trust\",\n",
    "    \"21\": \"Comparative Studies\",\n",
    "    \"22\": \"Content Analyses\",\n",
    "    \"23\": \"Policy and Research Reviews\",\n",
    "    \"24\": \"Profiling and Micro-Targeting\",\n",
    "    \"25\": \"Advertising\",\n",
    "    \"26\": \"Research Access\",\n",
    "    \"27\": \"Chinese Surveillance\",\n",
    "    \"28\": \"Data Trade\",\n",
    "    \"29\": \"Deepfakes\",\n",
    "    \"30\": \"Comparative Studies\",\n",
    "    \"31\": \"Data Competition\",\n",
    "    \"32\": \"Platform Regulation\",\n",
    "    \"33\": \"Data Asymmetries\",\n",
    "    \"34\": \"(Digital) Human Rights\",\n",
    "    \"35\": \"Data Governance\",\n",
    "    \"36\": \"Hate Speech\",\n",
    "    \"37\": \"Privacy Regulation\",\n",
    "    \"38\": \"Privacy Regulation\",\n",
    "    \"39\": \"Information Freedom\",\n",
    "    \"40\": \"Governance Gap\",\n",
    "    \"41\": \"AI Disinformation Risk\",\n",
    "    \"42\": \"Middle East Disinformation\",\n",
    "    \"43\": \"Digital Inclusion\",\n",
    "    \"44\": \"Governance Gap\",\n",
    "    \"45\": \"Algorithmic Politics\",\n",
    "    \"46\": \"Algorithmic Content Moderation\",\n",
    "    \"47\": \"Content Moderation\",\n",
    "    \"48\": \"LLMs\",\n",
    "    \"49\": \"Gen-AI News and Journalism\",\n",
    "    \"50\": \"Disinformation Campaigns\",\n",
    "    \"51\": \"Political Communication\",\n",
    "    \"52\": \"AI Governance\",\n",
    "    \"53\": \"Democratizing AI\",\n",
    "    \"54\": \"AI Definitions\",\n",
    "    \"55\": \"Disinformation Campaigns\",\n",
    "    \"56\": \"Disinformation Campaigns\",\n",
    "    \"57\": \"News Media Bias\",\n",
    "    \"58\": \"Decolonisation\",\n",
    "    \"59\": \"Twitter Research\",\n",
    "    \"60\": \"WhatsApp Research\",\n",
    "    \"61\": \"Media Regulation\",\n",
    "    \"62\": \"African Media\",\n",
    "    \"63\": \"Digital News and Journalism\",\n",
    "    \"64\": \"Digital News and Journalism\",\n",
    "    \"65\": \"Digital News and Journalism\",\n",
    "    \"66\": \"Digital News and Journalism\",\n",
    "    \"67\": \"Russian Trolls\",\n",
    "    \"68\": \"Political Campaigns and Elections\",\n",
    "    \"69\": \"Gender Media and Politics\",\n",
    "    \"70\": \"Populism and Authoritarianism\",\n",
    "    \"71\": \"Polarization\"\n",
    "}\n",
    "\n",
    "dictionary_second_level = {'Unlabelled':'Unlabelled',\n",
    "    \"AI Fairness and AI Bias\": \"Artificial Intelligence\",\n",
    "    \"AI News and Journalism\": \"Artificial Intelligence\",\n",
    "    \"Deepfakes\": \"Artificial Intelligence\",\n",
    "    'AI and Data Governance Objectives':'Artificial Intelligence',\n",
    "    \"AI Disinformation Risk\": \"Artificial Intelligence\",\n",
    "    \"Algorithmic Politics\": \"Artificial Intelligence\",\n",
    "    \"LLMs\": \"Artificial Intelligence\",\n",
    "    \"Gen-AI News and Journalism\": \"Artificial Intelligence\",\n",
    "    \"AI Governance\": \"Artificial Intelligence\",\n",
    "    \"Democratizing AI\": \"Artificial Intelligence\",\n",
    "    \"AI Definitions\": \"Artificial Intelligence\",\n",
    "    \"Journalists Online Harassment\": \"Content Moderation\",\n",
    "    \"Platform Regulation\": \"Content Moderation\",\n",
    "    \"Hate Speech\": \"Content Moderation\",\n",
    "    \"Information Freedom\": \"Content Moderation\",\n",
    "    \"Algorithmic Content Moderation\": \"Content Moderation\",\n",
    "    \"Chinese Surveillance\": \"Data\",\n",
    "    \"Data Competition\": \"Data\",\n",
    "    \"Data Asymmetries\": \"Data\",\n",
    "    \"Data Governance\": \"Data\",\n",
    "    \"Privacy Regulation\": \"Data\",\n",
    "    \"Governance Gap\": \"Data\",\n",
    "    \"Digital Inclusion\": \"Data\",\n",
    "    \"(Digital) Media Literacy\": \"News Media\",\n",
    "    \"Digital News and Journalism\": \"News Media\",\n",
    "    \"Children Internet Use\": \"News Media\",\n",
    "    \"Political News Consumption\": \"News Media\",\n",
    "    \"Media Trust\": \"News Media\",\n",
    "    \"News Media Bias\": \"News Media\",\n",
    "    \"Socio-demographic Indicators\": \"News Media\",\n",
    "    \"Media Regulation\": \"News Media\",\n",
    "    \"African Media\": \"News Media\",\n",
    "    \"Comparative Studies\": \"Research and Methodologies\",\n",
    "    \"Content Analyses\": \"Research and Methodologies\",\n",
    "    \"Policy and Research Reviews\": \"Research and Methodologies\",\n",
    "    \"Mis- and Disinformation\": \"Mis- and Disinformation\",\n",
    "    \"Fact Checking\": \"Mis- and Disinformation\",\n",
    "    \"Fake News and Politicization\": \"Mis- and Disinformation\",\n",
    "    \"Covid-19 Misinformation\": \"Mis- and Disinformation\",\n",
    "    \"Middle East Disinformation\": \"Mis- and Disinformation\",\n",
    "    \"Disinformation Campaigns\": \"Mis- and Disinformation\",\n",
    "    \"Russian Trolls\": \"Mis- and Disinformation\",\n",
    "    \"Political Communication\": \"Social Media and Politics\",\n",
    "    \"Political Campaigns and Elections\": \"Social Media and Politics\",\n",
    "    \"Gender Media and Politics\": \"Social Media and Politics\",\n",
    "    \"Populism and Authoritarianism\": \"Social Media and Politics\",\n",
    "    \"Polarization\": \"Social Media and Politics\",\n",
    "    \"Twitter Research\": \"Social Media and Politics\",\n",
    "    \"WhatsApp Research\": \"Social Media\",\n",
    "    \"Profiling and Micro-Targeting\": \"Content Moderation\",\n",
    "    \"Advertising\": \"Unlabelled\",\n",
    "    \"Research Access\": \"Research and Methodologies\",\n",
    "    \"(Digital) Human Rights\": \"Unlabelled\",\n",
    "    \"Zero Rating and Net Neutrality\": \"Zero Rating and Net Neutrality\",\n",
    "    'AI and Data Governance Objectives':'Artificial Intelligence', \n",
    "    'Decolonisation': 'Research and Methodologies',\n",
    "    'Content Moderation':'Content Moderation', \n",
    "    'Data Trade':'Data'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ab814",
   "metadata": {},
   "source": [
    "# Step 4 - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d84918",
   "metadata": {},
   "source": [
    "### Prep data\n",
    "We first map all cluster names (defined on two levels – macro and micro clusters) to their respective chunks, and we add the regional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_df = averaged_df.copy()\n",
    "\n",
    "visualization_df['map_cluster']=visualization_df['map_cluster'].astype(str)\n",
    "visualization_df['labels_1']=visualization_df['map_cluster'].map(dictionary)\n",
    "visualization_df['labels_2']=visualization_df['labels_1'].map(dictionary_second_level)\n",
    "\n",
    "# Check that no label is na – both outputs should be 0\n",
    "print('We have the following number of rows containing na labels:')\n",
    "print(len(visualization_df[visualization_df.labels_1.isna()==True]))\n",
    "print(len(visualization_df[visualization_df.labels_2.isna()==True]))\n",
    "\n",
    "# add regional information from the main df\n",
    "visualization_df = pd.merge(visualization_df,df[['Title', 'region']], on='Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245e260",
   "metadata": {},
   "source": [
    "We then prepare all the variables in the right format we need for the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a59f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates \n",
    "reduced_embedding=np.array(visualization_df.map_coordinates)\n",
    "reduced_embedding = np.array([list(map(float, re.sub(r\"(\\d)\\s+(-?\\d)\", r\"\\1, \\2\", item)\n",
    "                                .strip(\"[]\")\n",
    "                                .split(\", \"))) for item in reduced_embedding])\n",
    "\n",
    "#Topic labels\n",
    "labels = np.array(visualization_df['labels_1'], dtype=object)\n",
    "labels_big = np.array(visualization_df['labels_2'], dtype=object)\n",
    "\n",
    "#Hover Data and extra points data\n",
    "titles = np.array(visualization_df.Title)\n",
    "author = np.array(visualization_df.Authors.fillna('Unknown'))\n",
    "region = np.array(visualization_df.region.fillna('Other'))\n",
    "extra_data = pd.DataFrame(\n",
    "    {\"author\":author, \"region\":region}\n",
    ")\n",
    "\n",
    "#Hover color data\n",
    "color_mapping = {}\n",
    "color_mapping[\"Global\"] = \"#a64531\"\n",
    "color_mapping[\"Global North\"] = \"#ff0000\"\n",
    "color_mapping[\"Global Majority\"] = \"#59bace\"\n",
    "color_mapping[\"Other\"] = \"#6c6c6c\"\n",
    "extra_data[\"color\"] = extra_data.region.map(color_mapping)\n",
    "marker_color_array = extra_data.region.map(color_mapping)\n",
    "\n",
    "#Histogram Variables\n",
    "visualization_df[\"Publication Year\"]=pd.to_datetime(visualization_df[\"Publication Year\"].fillna('2024').astype(int), format='%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d23a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_df[\"Publication Year\"].unique() # Check that dates are in the right format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151c721",
   "metadata": {},
   "source": [
    "### Prep additional HTML, CSS and JavaScript Components\n",
    "Here below we create the custom HTML for the **hover text** so that we can include mor info when we hover over the points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_css = \"\"\"\n",
    "    border-radius:6px;\n",
    "    width:fit-content;\n",
    "    max-width:75%;\n",
    "    margin:2px;\n",
    "    padding: 2px 10px 2px 10px;\n",
    "    font-size: 10pt;\n",
    "\"\"\"\n",
    "hover_text_template = f\"\"\"\n",
    "<div>\n",
    "    <div style=\"background-color:{{color}};color:#fff;{badge_css}\">{{region}}</div>\n",
    "    <div style=\"font-size:small;padding:2px;\">{{hover_text}}</div>\n",
    "    <div style=\"font-size:small;padding:2px;\"><b>Author</b>: {{author}}</div>\n",
    "</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6d07e",
   "metadata": {},
   "source": [
    "We hereby construct the **global north - global majority** selection button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Region Button – css layout\n",
    "custom_css=\"\"\"\n",
    ".row {\n",
    "    display : flex;\n",
    "    align-items : center;\n",
    "}\n",
    ".box {\n",
    "    height:10px;\n",
    "    width:10px;\n",
    "    border-radius:2px;\n",
    "    margin-right:5px;\n",
    "    padding:0px 0 1px 0;\n",
    "    text-align:center;\n",
    "    color: white;\n",
    "    font-size: 4px;\n",
    "    cursor: pointer;\n",
    "}\n",
    "#region_button {\n",
    "    position: absolute;\n",
    "    bottom: 0;\n",
    "    left: 0;\n",
    "}\n",
    "#title-container {\n",
    "    max-width: 85%;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "#### HTML Content\n",
    "custom_html = \"\"\"\n",
    "<div id=\"region_button\" class=\"container-box\">\n",
    "    <div style=\"font-size:11pt;padding:2px;\"><b>Select Region</b></div>\n",
    "\"\"\"\n",
    "for region, color in color_mapping.items():\n",
    "    custom_html += f'    <div class=\"row\" ><div id=\"{region}\" class=\"box\" style=\"background-color:{color};\"></div>{region}</div>\\n'\n",
    "custom_html += \"\"\"\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "#### JavaScript Activation\n",
    "custom_js = \"\"\"\n",
    "const region_button = document.getElementById(\"region_button\");\n",
    "const selectedRegions = new Set();\n",
    "\n",
    "// Update the displayed data based on selected categories\n",
    "function updateDisplayedData() {\n",
    "    const selectedIndices = [];\n",
    "    datamap.metaData.region.forEach((reg, i) => {\n",
    "        if (selectedRegions.has(reg)) {\n",
    "            selectedIndices.push(i);\n",
    "        }\n",
    "    });\n",
    "    datamap.addSelection(selectedIndices, \"region_button\");\n",
    "}\n",
    "\n",
    "// Add event listener to the button container\n",
    "region_button.addEventListener('click', function (event) {\n",
    "    // Ensure the clicked element has an ID and is a valid button\n",
    "    const selectedCategory = event.target.id;\n",
    "    if (!selectedCategory) return;\n",
    "\n",
    "    // Toggle the selection of the category\n",
    "    if (selectedRegions.has(selectedCategory)) {\n",
    "        selectedRegions.delete(selectedCategory);\n",
    "        event.target.innerHTML = \"\"; // Uncheck\n",
    "    } else {\n",
    "        selectedRegions.add(selectedCategory);\n",
    "        event.target.innerHTML = \"✓\"; // Check\n",
    "    }\n",
    "\n",
    "    // Update the data display\n",
    "    updateDisplayedData();\n",
    "});\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef414e6",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f939745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = datamapplot.create_interactive_plot(\n",
    "    reduced_embedding,\n",
    "    labels_big,\n",
    "    labels,\n",
    "    \n",
    "    # Labels parameters\n",
    "    text_collision_size_scale=3,\n",
    "    min_fontsize=8,\n",
    "    max_fontsize=28,\n",
    "    \n",
    "    # HoverText and search\n",
    "    hover_text = titles,\n",
    "    enable_search=True,\n",
    "    on_click=\"window.open(`http://google.com/search?q=\\\"{hover_text}\\\"`)\",\n",
    "    \n",
    "    # Title and Subtitle\n",
    "    font_family=\"Montserrat\",\n",
    "    title=\"Information and Democracy Semantic Landscape\",\n",
    "    sub_title=\"A data map of literature from the Observatory on Information and Democracy's first research cycle\",\n",
    "    \n",
    "    #Layout\n",
    "    cluster_boundary_polygons=True,\n",
    "    cluster_boundary_line_width=2,\n",
    "    initial_zoom_fraction=6,\n",
    "    darkmode=True,\n",
    "\n",
    "    # Publication date histogram\n",
    "    histogram_data=visualization_df[\"Publication Year\"],\n",
    "    histogram_group_datetime_by=\"year\",\n",
    "    histogram_range=(pd.to_datetime(\"2000-01-01\"), pd.to_datetime(\"2025-08-08\")),\n",
    "    histogram_settings={\n",
    "        \"histogram_log_scale\":False,\n",
    "        \"histogram_title\":\"Publication Year\",\n",
    "        \"histogram_bin_fill_color\":\"#a64531\",\n",
    "        \"histogram_bin_unselected_fill_color\":\"#d6a591\",\n",
    "        \"histogram_bin_selected_fill_color\":\"#f68571\",\n",
    "        \"histogram_width\":300,\n",
    "        \"histogram_height\":100,\n",
    "    },\n",
    "    \n",
    "    # Add information for hovering\n",
    "    extra_point_data=extra_data,\n",
    "    hover_text_html_template=hover_text_template,\n",
    "    \n",
    "    # Add Global north - Global Majority Button\n",
    "    custom_css=custom_css,\n",
    "    custom_html=custom_html,\n",
    "    custom_js=custom_js,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save('OID_semantic_landscape_FINAL.html') # Save map as html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
