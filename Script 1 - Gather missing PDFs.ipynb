{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc27dbc",
   "metadata": {},
   "source": [
    "# Gather Missing PDFs\n",
    "\n",
    "While Zotero does a good job at saving PDF files, a lot of them were still missing in the file attachment column. Before conducting the data analysis we thus made sure to collect as many missing PDFs as we could. After running the script we were able to gather **around 90%** of all missing PDFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from doi2pdf import doi2pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ef2a5",
   "metadata": {},
   "source": [
    "1. Import the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.../OID_library.csv')\n",
    "\n",
    "df_nopdf = df[df['File Attachments'].isna()]\n",
    "\n",
    "print(f'Out of the {len(df_nopdf)} articles or entries that do not have a pdf attached '\n",
    "      f'{len(df_nopdf[df_nopdf.DOI.isna()])} do not have a DOI, while {len(df_nopdf[df_nopdf.Url.isna()])} '\n",
    "      f'do not have a Url associated with them.')\n",
    "print()\n",
    "print('In total, theoretically, we can download ',len(df_nopdf[~df_nopdf.DOI.isna()]['DOI']),\n",
    "      ' through the DOI and ', len(list(df_nopdf[~df_nopdf.Url.isna()]['Url'])), ' with the URL')\n",
    "\n",
    "df_nopdf[['Title','Author','Url','DOI']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfa365",
   "metadata": {},
   "source": [
    "2. Gain PDFs with request and bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfdf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_from_url(url, title):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=15)  # Set a timeout of 15 seconds\n",
    "        response.raise_for_status()  # Raise an error for HTTP request issues\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for link in soup.select(\"a[href$='.pdf']\"):\n",
    "            filename = f'{title}.pdf'\n",
    "            path = '.../pdf_gathering/additional_pdfs/'\n",
    "            output = path + filename\n",
    "            \n",
    "            pdf_url = urljoin(url, link['href'])\n",
    "            pdf_response = requests.get(pdf_url, timeout=15)  # Timeout for the PDF request\n",
    "            pdf_response.raise_for_status()\n",
    "            \n",
    "            with open(output, 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout reached for URL: {url}. Moving to the next URL.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing URL: {url} - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61278cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0 # I simply use this to count the number ot processed pdfs\n",
    "urls=list(df_nopdf[~df_nopdf.Url.isna()]['Url'])\n",
    "url_titles=list(df_nopdf[~df_nopdf.Url.isna()]['Title'])\n",
    "\n",
    "for url, title in zip(urls, url_titles):\n",
    "    i+=1\n",
    "    try:\n",
    "        get_pdf_from_url(url,title)\n",
    "        print(i,'Successfully downloaded', title)\n",
    "    except Exception as e:\n",
    "        print('Error downloading',title, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05063ce",
   "metadata": {},
   "source": [
    "3. Gain PDFs with doi2pdf (If a DOI is a dupliate of an already downloaded paper it will be retained only once - the title is the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b89c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dois=list(df_nopdf[~df_nopdf.DOI.isna()]['DOI'])\n",
    "titles=list(df_nopdf[~df_nopdf.DOI.isna()]['Title'])\n",
    "\n",
    "for doi, title in zip(dois, titles):\n",
    "    try:\n",
    "        doi2pdf(doi, output=f'.../pdf_gathering/additional_pdfs/{title}.pdf')\n",
    "    except Exception as e:\n",
    "        print(title, e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
